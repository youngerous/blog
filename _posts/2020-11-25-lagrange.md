---
title: "Lagrange Multiplier"
categories: 
  - optimization
last_modified_at: 2020-11-25
tags:
  - lagrangian
toc: true
use_math: true
---

# Lagrange Multiplier(라그랑주 승수법)

## 제약식이 있는 최적화 문제

어떠한 함수의 최적점(최댓값 혹은 최솟값)을 찾아야 한다. 다만, 등식 혹은 부등식 제약조건이 존재한다.


간단한 예시로 $f(x,y)=x^2y$의 함수와 $x^2+y^2=1$이라는 제약조건이 있을 때, 최적점은 접점(tangent)에서 발생한다. 여기서 최적화를 위해서는 Gradient Vector를 이용하는데, Gradient Vector는 접선벡터와 수직(perpendicular)이다([참고자료](https://m.blog.naver.com/PostView.nhn?blogId=mindo1103&logNo=90153612595&proxyReferer=https:%2F%2Fwww.google.com%2F)). 


따라서 아래와 같이 정리할 수 있다.

- 함수 $f(x,y)=x^2y$ 위 한 점에서의 Gradient Vector는 해당 점에서의 접선벡터와 수직이다.
- 제약조건 $x^2+y^2=1$ 위 한 점에서의 Gradient Vector는 해당 점에서의 접선벡터와 수직이다.
- 따라서 함수와 제약조건의 접점에서 Gradient Vector를 구하면, 두 Gradient Vector의 방향은 같다(길이가 같다는 보장은 없지만 상수배로 표현할 수 있다).


이를 식으로 표현하면 아래와 같다. 여기서 $\lambda$로 표현되는 상수를 Lagrange Multiplier라고 부른다.

$$ \nabla f(x_m, y_m) = \lambda \nabla g(x_m, y_m) \;\; \cdots \;\; ①$$


위의 예시를 그대로 사용하여 gradient를 구해보자.

$$ \nabla f = \nabla (x^2y) = \begin{bmatrix}
2xy \\
x^2
\end{bmatrix} $$


$$ \nabla g = \nabla (x^2+y^2) = \begin{bmatrix}
2x \\
2y
\end{bmatrix} $$


식 ①에 따르면 이 둘의 관계는 아래와 같이 표현할 수 있다. 

$$ 2xy = \lambda 2x $$

$$ x^2 = \lambda 2y $$

미지수가 총 3개($x, y, \lambda$)이기 때문에 방정식이 하나 더 필요한데, 처음의 제약식($x^2+y^2=1$)을 사용하면 된다.


## 라그랑지안 식

가독성을 위해 함수와 제약식을 일반화하여 다시 적어두겠다.

- 목적함수: $f(x,y)$
- 제약조건: $g(x,y) = c$ (단, $c$는 상수)

이 최적화 과정을 아래의 식으로 한 번에 표현할 수 있다.

$$L(x,y,\lambda) = f(x,y) - \lambda(g(x,y)-c) \;\; \cdots \;\; ②$$

그리고 아래와 같이 **식 ②의 편미분값($\nabla L$)이 0** 이 되는 값을 찾으면 된다.

$$ \begin{bmatrix}
\partial L/\partial x \\
\partial L/\partial y \\
\partial L/\partial \lambda \\
\end{bmatrix} =
 \begin{bmatrix}
0 \\
0 \\
0 \\
\end{bmatrix}$$



그 이유에 대해 직관적으로 받아들이기가 쉽지 않지만, 아래와 같은 과정을 거치면 이해할 수 있다.

### 1. $x$에 대한 편미분

$$ {\partial L \over \partial x} =  {\partial f \over \partial x} -  \lambda{\partial g \over \partial x} = 0 $$

이는 식 ①을 만족한다.

### 2. $y$에 대한 편미분

$$ {\partial L \over \partial y} =  {\partial f \over \partial y} -  \lambda{\partial g \over \partial y} = 0 $$

이는 식 ①을 만족한다.

### 3. $\lambda$에 대한 편미분

$$ {\partial L \over \partial \lambda} =  -(g(x,y)-c)=0 $$

이는 제약조건을 만족한다.


## Lagrange Multiplier가 갖는 의미
드디어 라그랑주 승수법을 사용하여 얻어진 최적값($x^*, y^*$)을 목적함수에 대입하여 구하고자 하는 결과를 얻을 수 있다. 그렇다면 구해진 최적의 $\lambda^*$가 갖는 의미는 무엇일까? 결론적으로 말하자면 $\lambda^*$는 단순히 Gradient Vector 간 비율만을 의미하는 것이 아니라 **제약조건을 변화시켰을 때 목적함수의 값이 얼마나 변하는지**를 나타낸다. 


이 역시 직관적으로 받아들이기 힘들기 때문에 아래와 같은 과정을 통해 이해한다.

### 1. 라그랑지안 함수 재정의

이전까지는 $c$를 상수로 정의했지만, $c$의 변화에 따른 결과를 확인하고 싶기 때문에 식 ②를 $c$를 변수로 정의하여 아래와 같은 식으로 다시 표현할 수 있다.

$$L(x,y,\lambda, c) = f(x,y) - \lambda(g(x,y)-c)$$


### 2. 최적값에 대한 라그랑지안 함수 표현
최적값이 들어간 라그랑지안 식은 아래와 같이 표현할 수 있다. 모든 최적값($x^*, y^*, \lambda^*$)은 $c$에 종속적이기 때문에 $c$를 명시하였다. 이 식은 $c$에 대한 단변량함수로도 볼 수 있다.

$$L^*(x^*(c),y^*(c),\lambda^*(c), c) = f(x^*(c),y^*(c)) - \lambda^*(c)(g(x^*(c),y^*(c))-c) $$

### 3. $c$에 대한 미분값 구하기

이제 $c$에 대한 $L$의 미분값을 구할 것이다. Multivariate Chain Rule에 의해 아래와 같이 표현된다. $L$과 $L^*$의 식의 형태는 동일하기 때문에 다변량함수인 $L$에 대한 Chain Rule로 표시할 것이다.

$$ {dL^*\over dc} = {\partial L \over \partial x^*}\cdot {\partial x^* \over \partial c} + {\partial L \over \partial y^*}\cdot {\partial y^* \over \partial c} + {\partial L \over \partial \lambda^*}\cdot {\partial \lambda^* \over \partial c} + {\partial L \over \partial c}\cdot {\partial c \over \partial c}  $$

그런데 라그랑지안 식의 정의에 따르면 $x^*, y^*, \lambda^*$에 대한 편미분값은 0이기 때문에 결국 아래와 같은 식이 도출된다.

$$ {dL^*\over dc} =  {\partial L \over \partial c} = \lambda =  \lambda^*(c)$$
  

즉, $\lambda^*$는 제약조건의 값 $c$의 변화에 따른 최적값의 변화량을 의미하는 것이다.

## 요약
- 제약식이 있는 목적함수를 제약식이 없는 라그랑지안 함수로 바꾸어 준다면, 굳이 조건을 하나하나 확인하지 않고 극점을 찾을 수 있으면서도 기존의 제약조건을 만족할 수 있다. 
- 라그랑주 승수는 제약조건의 값 변화에 따른 최적값의 변화량을 의미한다.

## 참고자료 
- [Khan Academy: Lagrange multipliers and constrained optimization](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/constrained-optimization-introduction)
- [기울기 벡터(Gradient Vector)의 기하학적 의미](https://m.blog.naver.com/PostView.nhn?blogId=mindo1103&logNo=90153612595&proxyReferer=https:%2F%2Fwww.google.com%2F)
  